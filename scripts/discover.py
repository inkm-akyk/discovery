import requests
from bs4 import BeautifulSoup
from datetime import datetime
import re

print("=== Fashion Discovery System ===")
print(f"å®Ÿè¡Œæ™‚åˆ»: {datetime.now()}")
print("")

try:
    # Makuakeãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‹ã‚‰æ–°ç€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’æ¢ã™
    url = "https://www.makuake.com/"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    response = requests.get(url, headers=headers, timeout=10)
    
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¸ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
        all_links = soup.find_all('a', href=re.compile(r'/project/'))
        
        # é‡è¤‡ã‚’é™¤å»
        project_urls = []
        seen = set()
        for link in all_links:
            href = link.get('href')
            if href and href not in seen and '/project/' in href:
                if not href.startswith('http'):
                    href = 'https://www.makuake.com' + href
                seen.add(href)
                project_urls.append({
                    'url': href,
                    'text': link.get_text(strip=True)[:100]  # æœ€åˆã®100æ–‡å­—
                })
        
        print(f"âœ… {len(project_urls)}ä»¶ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç™ºè¦‹")
        print("")
        
        # æœ€åˆã®5ä»¶ã‚’è¡¨ç¤º
        for i, proj in enumerate(project_urls[:5], 1):
            print(f"{i}. {proj['text']}")
            print(f"   ğŸ”— {proj['url']}")
            print("")
            
    else:
        print(f"âš ï¸ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}")
        
except Exception as e:
    print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
    import traceback
    traceback.print_exc()

print("=== å®Œäº† ===")
