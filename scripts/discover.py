import requests
from datetime import datetime, timedelta
import os
import json
import feedparser
from googletrans import Translator
import time

# ç¿»è¨³å™¨ã®åˆæœŸåŒ–
translator = Translator()

def translate_text(text, max_retries=3):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’æ—¥æœ¬èªã«ç¿»è¨³ï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãï¼‰"""
    if not text:
        return ""

    for attempt in range(max_retries):
        try:
            result = translator.translate(text, src='en', dest='ja')
            return result.text
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(1)  # 1ç§’å¾…ã£ã¦ãƒªãƒˆãƒ©ã‚¤
                continue
            print(f"ç¿»è¨³ã‚¨ãƒ©ãƒ¼: {e}")
            return text  # ç¿»è¨³å¤±æ•—æ™‚ã¯åŸæ–‡ã‚’è¿”ã™

def fetch_hacker_news(days=7, min_score=100):
    """Hacker Newsã‹ã‚‰éå»Næ—¥é–“ã®é«˜è©•ä¾¡è¨˜äº‹ã‚’å–å¾—"""
    print("ğŸ“° Hacker Newsã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ä¸­...")

    try:
        # HN Algolia APIã‚’ä½¿ç”¨ï¼ˆéå»7æ—¥é–“ã®ãƒˆãƒƒãƒ—è¨˜äº‹ï¼‰
        cutoff_timestamp = int((datetime.now() - timedelta(days=days)).timestamp())
        url = f"https://hn.algolia.com/api/v1/search?tags=story&numericFilters=created_at_i>{cutoff_timestamp},points>{min_score}&hitsPerPage=10"

        response = requests.get(url, timeout=10)
        data = response.json()

        articles = []
        for hit in data.get('hits', [])[:5]:  # ä¸Šä½5ä»¶
            title = hit.get('title', '')
            url = hit.get('url') or f"https://news.ycombinator.com/item?id={hit.get('objectID')}"
            points = hit.get('points', 0)
            comments = hit.get('num_comments', 0)

            # ç¿»è¨³
            title_ja = translate_text(title)

            articles.append({
                'source': 'Hacker News',
                'title': title,
                'title_ja': title_ja,
                'url': url,
                'score': points,
                'comments': comments
            })

        print(f"  âœ… {len(articles)}ä»¶å–å¾—")
        return articles

    except Exception as e:
        print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def fetch_reddit_rss(subreddit, limit=5):
    """Redditã®RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰é«˜è©•ä¾¡æŠ•ç¨¿ã‚’å–å¾—"""
    print(f"ğŸ“± r/{subreddit}ã‹ã‚‰æŠ•ç¨¿ã‚’å–å¾—ä¸­...")

    try:
        # Reddit RSSãƒ•ã‚£ãƒ¼ãƒ‰ï¼ˆãƒˆãƒƒãƒ—æŠ•ç¨¿ï¼‰
        rss_url = f"https://www.reddit.com/r/{subreddit}/top/.rss?t=week"
        feed = feedparser.parse(rss_url)

        posts = []
        for entry in feed.entries[:limit]:
            title = entry.title
            url = entry.link

            # ç¿»è¨³
            title_ja = translate_text(title)

            posts.append({
                'source': f'r/{subreddit}',
                'title': title,
                'title_ja': title_ja,
                'url': url,
                'score': 0,  # RSSã‹ã‚‰ã¯æ­£ç¢ºãªæ•°å€¤å–å¾—å›°é›£
                'comments': 0
            })

        print(f"  âœ… {len(posts)}ä»¶å–å¾—")
        return posts

    except Exception as e:
        print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def send_slack_notification(all_discoveries):
    """Slack Webhook URLã«é€šçŸ¥ã‚’é€ä¿¡"""
    webhook_url = os.environ.get('SLACK_WEBHOOK_URL')

    if not webhook_url:
        print("âš ï¸ SLACK_WEBHOOK_URL ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return False

    # ç·ä»¶æ•°ã‚’è¨ˆç®—
    total_count = sum(len(items) for items in all_discoveries.values())

    # Slack ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ§‹ç¯‰
    blocks = [
        {
            "type": "header",
            "text": {
                "type": "plain_text",
                "text": "ğŸ” é€±æ¬¡Discovery Report",
                "emoji": True
            }
        },
        {
            "type": "section",
            "text": {
                "type": "mrkdwn",
                "text": f"*{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}*\nä»Šé€±ç™ºè¦‹: *{total_count}ä»¶*"
            }
        },
        {
            "type": "divider"
        }
    ]

    # å„ã‚½ãƒ¼ã‚¹ã”ã¨ã«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆ
    for source_name, items in all_discoveries.items():
        if not items:
            continue

        # ã‚½ãƒ¼ã‚¹ãƒ˜ãƒƒãƒ€ãƒ¼
        blocks.append({
            "type": "section",
            "text": {
                "type": "mrkdwn",
                "text": f"*ã€{source_name}ã€‘ {len(items)}ä»¶*"
            }
        })

        # å„è¨˜äº‹
        for i, item in enumerate(items, 1):
            title_ja = item.get('title_ja', item.get('title', ''))
            title_en = item.get('title', '')
            url = item.get('url', '')
            score = item.get('score', 0)
            comments = item.get('comments', 0)

            # ãƒ¡ã‚¿æƒ…å ±
            meta = []
            if score > 0:
                meta.append(f"â­ {score}")
            if comments > 0:
                meta.append(f"ğŸ’¬ {comments}")

            meta_str = " | ".join(meta) if meta else ""

            # ã‚¿ã‚¤ãƒˆãƒ«è¡¨ç¤ºï¼ˆæ—¥æœ¬èªè¨³ + è‹±èªåŸæ–‡ï¼‰
            title_display = f"*{title_ja}*\n_{title_en}_"

            blocks.append({
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": f"{i}. {title_display}\n{meta_str}\n<{url}|è¨˜äº‹ã‚’è¦‹ã‚‹>"
                }
            })

        blocks.append({"type": "divider"})

    payload = {"blocks": blocks}

    try:
        response = requests.post(
            webhook_url,
            data=json.dumps(payload),
            headers={'Content-Type': 'application/json'},
            timeout=10
        )

        if response.status_code == 200:
            print("âœ… Slacké€šçŸ¥ã‚’é€ä¿¡ã—ã¾ã—ãŸ")
            return True
        else:
            print(f"âš ï¸ Slacké€šçŸ¥å¤±æ•—: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}")
            return False

    except Exception as e:
        print(f"âŒ Slacké€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")
        return False


print("=== Weekly Discovery System ===")
print(f"å®Ÿè¡Œæ™‚åˆ»: {datetime.now()}")
print("")

try:
    all_discoveries = {}

    # 1. Hacker News
    hn_articles = fetch_hacker_news(days=7, min_score=100)
    if hn_articles:
        all_discoveries['Hacker News'] = hn_articles

    # 2. r/BuyItForLife
    bifl_posts = fetch_reddit_rss('BuyItForLife', limit=5)
    if bifl_posts:
        all_discoveries['r/BuyItForLife'] = bifl_posts

    # 3. r/malefashionadvice
    mfa_posts = fetch_reddit_rss('malefashionadvice', limit=5)
    if mfa_posts:
        all_discoveries['r/malefashionadvice'] = mfa_posts

    # 4. r/LocalLLaMA
    llama_posts = fetch_reddit_rss('LocalLLaMA', limit=5)
    if llama_posts:
        all_discoveries['r/LocalLLaMA'] = llama_posts

    print("")
    print(f"âœ… åˆè¨ˆ {sum(len(v) for v in all_discoveries.values())}ä»¶ã®è¨˜äº‹ã‚’ç™ºè¦‹")
    print("")

    # Slackã«é€šçŸ¥
    if all_discoveries:
        send_slack_notification(all_discoveries)
    else:
        print("âš ï¸ è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

except Exception as e:
    print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
    import traceback
    traceback.print_exc()

print("=== å®Œäº† ===")
